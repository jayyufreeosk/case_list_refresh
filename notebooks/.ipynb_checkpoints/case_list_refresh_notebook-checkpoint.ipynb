{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Case-List-Refresh-Notebook\" data-toc-modified-id=\"Case-List-Refresh-Notebook-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Case List Refresh Notebook</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO\" data-toc-modified-id=\"TODO-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>TODO</a></span></li></ul></li><li><span><a href=\"#Hyperlink-work\" data-toc-modified-id=\"Hyperlink-work-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Hyperlink work</a></span></li><li><span><a href=\"#DRAFT\" data-toc-modified-id=\"DRAFT-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>DRAFT</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case List Refresh Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ask CSM about appropriate columns\n",
    "- Transfer knowledge to working framework\n",
    "- Work on WM portion\n",
    "    - Detail what columns to pull from SQL\n",
    "    - Create functionalities\n",
    "- Generalize functionalities for any network\n",
    "\n",
    "\n",
    "Thought process: Have the same outputs for both SC and WM and eventually Albertsons. This should be refreshed at least once a week, hopefully done by Jenkins or another chron tool. It should just run as a script rather than a program, though functionality should be given to do both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T16:43:07.084113Z",
     "start_time": "2020-03-13T16:43:07.068488Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "def mysql_query(query='DEFAULT', verbose=False):\n",
    "    import sqlalchemy as sql\n",
    "    username = 'jyu'\n",
    "    password = 't4pu6[\"Hw+X@=~gL'\n",
    "    connection = '192.168.157.106'\n",
    "    database_name = 'ods'\n",
    "    connect_string = f'mysql://{username}:{password}@{connection}/{database_name}'\n",
    "    sql_engine = sql.create_engine(connect_string)\n",
    "    df = pd.read_sql_query(query, sql_engine)\n",
    "    if verbose: print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T19:55:22.867284Z",
     "start_time": "2020-03-20T19:55:22.812432Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pygsheets\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# https://pygsheets.readthedocs.io/en/stable/worksheet.html\n",
    "def mysql_query(query='DEFAULT', verbose=False):\n",
    "    import sqlalchemy as sql\n",
    "    username = 'jyu'\n",
    "    password = 't4pu6[\"Hw+X@=~gL'\n",
    "    connection = '192.168.157.106'\n",
    "    database_name = 'ods'\n",
    "    connect_string = f'mysql://{username}:{password}@{connection}/{database_name}'\n",
    "    sql_engine = sql.create_engine(connect_string)\n",
    "    df = pd.read_sql_query(query, sql_engine)\n",
    "    if verbose: print(df.shape)\n",
    "    return df\n",
    "\n",
    "def gsheet_import_caselist(network, verbose=False):\n",
    "    # Relating network to sheet index\n",
    "    network_dict = {\n",
    "        \"Sam's Club\":0,\n",
    "        'Walmart':1\n",
    "    }\n",
    "    network_choice = network_dict[network]\n",
    "    \n",
    "    gc = pygsheets.authorize(client_secret='../client_secret.json')\n",
    "    sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1wsnBd3AHObl4gnUJ2MlwkusuXRoOsQpu2Kx6zGH8bVY/edit#gid=0')\n",
    "    df = sh[network_choice].get_as_df()\n",
    "    \n",
    "    if verbose: print(df.shape)\n",
    "    \n",
    "    print(f'Successfully returned {network}.')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gsheet_programs(gsheet_df):\n",
    "    print(f'Imported Gsheet Dataframe has (rows, columns): {gsheet_df.shape}')\n",
    "    not_list = gsheet_df['Program'].to_list() + ['0', '0']\n",
    "    not_tuple = tuple(not_list)\n",
    "    return not_tuple\n",
    "\n",
    "def sc_sql_metrics(not_tuple):\n",
    "    query = f\"\"\"\n",
    "    SELECT metrics.program, metrics.weeks_post, metrics.start_date, metrics.end_date, \n",
    "    metrics.item_number, metrics.item_description, metrics.category, metrics.subcategory,\n",
    "    metrics.category_num, metrics.subcat_num, metrics.Freeosk_clubs, metrics.control_clubs,\n",
    "    metrics.transactions, metrics.scans, metrics.audience, metrics.engagement, metrics.conversion,\n",
    "    metrics.repeat, metrics.Immediate, metrics.`HH_A%%`, metrics.`HH_B%%`, metrics.`HH_C%%`,\n",
    "    lift.freeosk_lift, lift.control_lift, cfg.merchandising_type, cfg.placement_type\n",
    "    FROM longitudinal.c_metrics as metrics\n",
    "    JOIN longitudinal.c_lift as lift ON metrics.Program = lift.program_name\n",
    "    JOIN longitudinal.c_cfg as cfg on metrics.program = cfg.program_name\n",
    "    WHERE metrics.Weeks_post = '12W'\n",
    "    AND metrics.program NOT IN {not_tuple}\n",
    "    AND metrics.program LIKE '%%_12.zip%%';\n",
    "    \"\"\"\n",
    "    raw_sc_df = mysql_query(query)\n",
    "    print(f'Number of SC rows to append: {len(raw_sc_df.index)}')\n",
    "    return raw_sc_df\n",
    "\n",
    "def sc_formatter(raw_sc_df):\n",
    "    # Formatting\n",
    "    print(f'Successfully returned the MySQL query')\n",
    "    sc_df = raw_sc_df.copy()\n",
    "    sc_df['Traffic/club'] = sc_df['audience'] / sc_df['Freeosk_clubs']\n",
    "    sc_df['Scans/club'] = sc_df['scans'] / sc_df['Freeosk_clubs']\n",
    "    sc_df['Program2'] = sc_df['program'].str.split(\"_\", n=1, expand=True)[0]\n",
    "    sc_df['Unique Identifier'] = sc_df['Program2'] + sc_df['item_number'].astype(str)\n",
    "    renamed_columns = {\n",
    "        'HH_A%': 'A%',\n",
    "        'HH_B%': 'B%',\n",
    "        'HH_C%': 'C%'\n",
    "    }\n",
    "    sc_df = sc_df.rename(columns=renamed_columns)\n",
    "    sc_df.loc[:, 'Tags'] = ''\n",
    "    sc_df.loc[:, 'Notes'] = ''\n",
    "    sc_df.columns = [column.replace('_', ' ') for column in list(sc_df)]\n",
    "    sc_df.columns = [column.capitalize() for column in list(sc_df)]\n",
    "    print(f'Successfully formatted the MySQL query')\n",
    "    \n",
    "    # Mapping to food boolean\n",
    "    food_mapping = pd.read_excel('data/Food_Mapping.xlsx')\n",
    "    sc_df_2 = pd.merge(sc_df, food_mapping,  how='left', \n",
    "                       left_on=['Category num','Subcat num'], \n",
    "                       right_on = ['CATEGORY_NUMBER','SUB_CATEGORY_NUMBER'])\n",
    "    print(f'Successfully joined to Food_Mapping.xslx')\n",
    "    \n",
    "    # Final column rearrangement\n",
    "    rearranged_columns = ['Program', 'Tags', 'Notes', 'Weeks post', 'Start date', \n",
    "                      'End date', 'Item number', 'Item description', 'Category', \n",
    "                      'Subcategory', 'Category num', 'Subcat num', 'Freeosk clubs', \n",
    "                      'Control clubs', 'Transactions', 'Audience', 'Traffic/club', \n",
    "                      'Scans', 'Scans/club', 'Engagement', 'Conversion', 'Repeat', \n",
    "                      'Immediate', 'A%', 'B%', 'C%', 'Freeosk lift', 'Control lift', \n",
    "                      'Merchandising type', 'Placement type', 'Program2', 'Unique identifier', 'Food']\n",
    "    \n",
    "    sc_df_3 = sc_df_2[rearranged_columns].reset_index(drop=True)\n",
    "    \n",
    "    print(f'SC_Formatter completed! Shape is {sc_df_3.shape}')\n",
    "    \n",
    "    return sc_df_3  \n",
    "\n",
    "def wm_sql_metrics(not_tuple):\n",
    "    query = f\"\"\"SELECT traceable.Program_name, \n",
    "    traceable.Weeks_post, traceable.Start_date, traceable.End_date,\n",
    "    traceable.Item_number, traceable.Placement_Name, traceable.Dept,\n",
    "    traceable.Sub_Dept, traceable.Dept_Name, traceable.Sub_Dept_Name,\n",
    "    traceable.Freeosk_stores, traceable.Rest_of_Chain_Stores,\n",
    "    traceable.Traceable_Audience, traceable.Non_Traceable_Aud,\n",
    "    traceable.Audience, traceable.Scans, traceable.Total_Conv,\n",
    "    traceable.Repeat, traceable.A, traceable.B, traceable.C,\n",
    "    traceable.Households, traceable.`A%%`, traceable.`B%%`, traceable.`C%%`, traceable.`A+B%%`,\n",
    "    lift.Control_lift, lift.Freeosk_lift\n",
    "    FROM longitudinal.traceable as traceable\n",
    "    JOIN longitudinal.lift as lift ON traceable.program_name = lift.Program_name\n",
    "    WHERE traceable.Weeks_post = '4W'\n",
    "    AND traceable.program_name LIKE '%%_4.zip'\n",
    "    AND traceable.program_name NOT IN {not_tuple};\"\"\"\n",
    "    raw_wm_df = mysql_query(query)\n",
    "    print(f'Number of WM rows to append: {len(raw_wm_df.index)}')\n",
    "    return raw_wm_df\n",
    "\n",
    "def wm_formatter(raw_wm_df):\n",
    "    # Formatting\n",
    "    print(f'Successfully returned the MySQL query')\n",
    "    wm_df = raw_wm_df.copy()\n",
    "    wm_df['Traffic/store'] = wm_df['Audience'] / wm_df['Freeosk_stores']\n",
    "    wm_df['Scans/store'] = wm_df['Scans'] / wm_df['Freeosk_stores']\n",
    "    wm_df['Program2'] = wm_df['Program_name'].str.split(\"_\", n=1, expand=True)[0]\n",
    "    wm_df.loc[:, 'Tags'] = ''\n",
    "    wm_df.loc[:, 'Notes'] = ''\n",
    "    wm_df.columns = [column.replace('_', ' ') for column in list(wm_df)]\n",
    "    wm_df.columns = [column.capitalize() for column in list(wm_df)]\n",
    "    print(f'Successfully formatted the MySQL query')\n",
    "    \n",
    "    # Final column rearrangement\n",
    "    rearranged_columns = ['Program name', 'Tags', 'Notes', 'Weeks post', 'Start date', 'End date', \n",
    "                          'Item number', 'Placement name', 'Dept', 'Sub dept', 'Dept name', \n",
    "                          'Sub dept name', 'Freeosk stores', 'Rest of chain stores', 'Traceable audience', \n",
    "                          'Non traceable aud', 'Audience', 'Traffic/store', 'Scans', 'Scans/store', 'Total conv', \n",
    "                          'Repeat', 'A', 'B', 'C', 'Households', 'A%', 'B%', 'C%', 'A+b%', 'Control lift', \n",
    "                          'Freeosk lift', 'Program2']\n",
    "    \n",
    "    wm_df = wm_df[rearranged_columns].reset_index(drop=True)\n",
    "    \n",
    "    rename_col = {'A+b%':'A+B%', 'Program name': 'Program'}\n",
    "    wm_df = wm_df.rename(columns=rename_col)\n",
    "    print(f'WM_Formatter completed! Shape is {wm_df.shape}')\n",
    "    \n",
    "    return wm_df\n",
    "\n",
    "def gsheet_uploader(network, gsheet_df, df):\n",
    "    gsheet_import_appended = gsheet_df.append(df, ignore_index=True)\n",
    "    \n",
    "    network_dict = {\n",
    "        \"Sam's Club\":0,\n",
    "        'Walmart':1\n",
    "    }\n",
    "    network_choice = network_dict[network]\n",
    "    \n",
    "    gc = pygsheets.authorize(client_secret='client_secret.json')\n",
    "    sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1wsnBd3AHObl4gnUJ2MlwkusuXRoOsQpu2Kx6zGH8bVY/edit#gid=0')\n",
    "    sh[network_choice].clear('A2')\n",
    "    sh[network_choice].set_dataframe(gsheet_import_appended, 'A2', copy_index=False, copy_head=False, extend=False, fit=True, escape_formulae=True, nan='')\n",
    "    print('New data has been successfully uploaded!') \n",
    "\n",
    "def refresher(network_input):\n",
    "    \n",
    "    if network_input == \"Sam's Club\" or network_input == 'All':\n",
    "        print(\"Running for Sam's Club\")\n",
    "        gsheet_df = gsheet_import_caselist(\"Sam's Club\")\n",
    "        not_tuple = gsheet_programs(gsheet_df)\n",
    "        raw_sc_df = sc_sql_metrics(not_tuple)\n",
    "        if raw_sc_df.empty: print('No new SC rows to append. Stopping upload.')\n",
    "        else: \n",
    "            sc_df = sc_formatter(raw_sc_df)\n",
    "            gsheet_uploader(\"Sam's Club\", gsheet_df, sc_df)\n",
    "    \n",
    "    print('------------------------------------------------')\n",
    "\n",
    "    if network_input == 'Walmart' or network_input == 'All':\n",
    "        print(\"Running for Walmart\")\n",
    "        gsheet_df = gsheet_import_caselist('Walmart')\n",
    "        not_tuple = gsheet_programs(gsheet_df)\n",
    "        raw_wm_df = wm_sql_metrics(not_tuple)\n",
    "        if raw_wm_df.empty: print('No new WM rows to append. Stopping upload.')\n",
    "        else: \n",
    "            wm_df = wm_formatter(raw_wm_df)\n",
    "            gsheet_uploader(\"Walmart\", gsheet_df, wm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T19:55:35.797694Z",
     "start_time": "2020-03-20T19:55:22.942085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Running for Walmart\n",
      "Successfully returned Walmart.\n",
      "Imported Gsheet Dataframe has (rows, columns): (104, 33)\n",
      "Number of WM rows to append: 0\n",
      "No new WM rows to append. Stopping upload.\n"
     ]
    }
   ],
   "source": [
    "refresher('Walmart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T19:17:13.122663Z",
     "start_time": "2020-03-20T19:17:12.478126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Program</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>FSC14005_475453_20140201_12.zip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Program\n",
       "0                             None\n",
       "1                       google.com\n",
       "2                             None\n",
       "3                             None\n",
       "4                             None\n",
       "5  FSC14005_475453_20140201_12.zip"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc = pygsheets.authorize(client_secret='../client_secret.json')\n",
    "sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1wsnBd3AHObl4gnUJ2MlwkusuXRoOsQpu2Kx6zGH8bVY/edit#gid=0')\n",
    "df = sh[2].get_as_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T19:10:55.927830Z",
     "start_time": "2020-03-20T19:10:55.917857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperlink work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T22:28:59.837812Z",
     "start_time": "2020-03-20T22:28:59.832827Z"
    }
   },
   "outputs": [],
   "source": [
    "class myclass:\n",
    "    \n",
    "    def __init__(self, string2):\n",
    "        self.string2 = string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T22:29:04.845421Z",
     "start_time": "2020-03-20T22:29:04.838436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s123'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink_update('https://docs.google.com/spreadsheets/d/1wsnBd3AHObl4gnUJ2MlwkusuXRoOsQpu2Kx6zGH8bVY/edit#gid=0', 'Walmart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T22:33:06.551424Z",
     "start_time": "2020-03-20T22:33:06.532476Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class hyperlink_update:\n",
    "    \n",
    "    def __init__(self, gsheet_url, network):\n",
    "        self.gsheet_url = gsheet_url\n",
    "        self.network = network\n",
    "    \n",
    "    def worksheet(self):\n",
    "        self.gc = pygsheets.authorize(client_secret='../client_secret.json')\n",
    "        self.sh = gc.open_by_url(self.gsheet_url)\n",
    "        \n",
    "        network_dict = {\n",
    "        \"Sam's Club\":0,\n",
    "        'Walmart':1\n",
    "        }\n",
    "        \n",
    "        # Relating network to sheet index\n",
    "        network_choice = network_dict[self.network]\n",
    "        network_choice = 2 # testing\n",
    "\n",
    "        self.wk = sh[network_choice]\n",
    "        \n",
    "## THREE DICTIONARIES CREATED\n",
    "### 1. D that lists {programs: Cell location} that need to be hyperlinked (hyperlink_retriever)\n",
    "### 2. D that lists {programs: GDrive ID} currently in 'Cases' GDrive folder (file_list_agg)\n",
    "### 3. D that combines D1 and D2 {Cell Location: {program: GDrive ID}} (file_name_checker)\n",
    "\n",
    "    def hyperlink_retriever(self, network): # Retrieves list of programs to find in Cases GDrive folder\n",
    "        self.requires_hyperlink_d = {}\n",
    "\n",
    "        n_rows = self.wk.rows + 1\n",
    "        for i in range(2, n_rows):\n",
    "            cell = pygsheets.Cell(f'A{i}', worksheet=self.wk, cell_data=None)\n",
    "            \n",
    "            cell_value = self.wk.get_value(f'A{i}')\n",
    "            \n",
    "            if not cell.formula and cell_value: # Must have value but no hyperlink\n",
    "\n",
    "                print(f'Cell A{i} requires a hyperlink: {cell_value}')\n",
    "                \n",
    "                file_name = cell_value.split('.')[0] + '_case.pptx' # Name to help locate file names\n",
    "                \n",
    "                # {File Name PPTX: Cell Location}\n",
    "                self.requires_hyperlink_d[file_name] = f'A{i}'\n",
    "        \n",
    "        return self.requires_hyperlink_d\n",
    "    \n",
    "    def file_list_agg(self):\n",
    "        from pydrive.auth import GoogleAuth\n",
    "        from pydrive.drive import GoogleDrive\n",
    "\n",
    "        gauth = GoogleAuth()\n",
    "        gauth.LocalWebserverAuth()\n",
    "\n",
    "        drive = GoogleDrive(gauth)\n",
    "        self.file_list = {}\n",
    "\n",
    "        folder_files = drive.ListFile({'q': \"'1qQZpAhzmKoR7drBYh8kYWVS6GaMPCxr3' in parents\"}).GetList()\n",
    "        for file in folder_files:\n",
    "            print(file['title'])\n",
    "            \n",
    "            # {'File Name PPTX': 'File GDrive ID'}\n",
    "            self.file_list[file['title']] = file['id']\n",
    "    \n",
    "    def file_name_checker(self):\n",
    "        self.hyperlink_d = {}\n",
    "        \n",
    "        for file_name, file_id in self.file_list.items():\n",
    "            if file_name in self.requires_hyperlink_d.keys():\n",
    "                print(f'Found one {file_name}!')\n",
    "        \n",
    "                file_name_zip = file_name.split('.pptx')[0] + '.zip' # Reference back to original name\n",
    "            \n",
    "                # {'Cell location' : {'File Name Zipped': 'File GDrive ID'}}\n",
    "                self.hyperlink_d[self.requires_hyperlink_d[file_name]] = {file_name_zip:file_id} \n",
    "    \n",
    "    def hyperlink_updater(self):\n",
    "        for cell_location, inner_d in self.hyperlink_d.items():\n",
    "            file_name_zip = str(list(inner_d.keys())[0])\n",
    "            cell = pygsheets.Cell(cell_location, worksheet=self.wk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def hyperlink_retriever(network):\n",
    "    gc = pygsheets.authorize(client_secret='../client_secret.json')\n",
    "    sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1wsnBd3AHObl4gnUJ2MlwkusuXRoOsQpu2Kx6zGH8bVY/edit#gid=0')\n",
    "    \n",
    "    # Relating network to sheet index\n",
    "    network_dict = {\n",
    "        \"Sam's Club\":0,\n",
    "        'Walmart':1\n",
    "    }\n",
    "    network_choice = network_dict[network]\n",
    "    \n",
    "    network_choice = 2 # testing\n",
    "    \n",
    "    wk = sh[network_choice]\n",
    "    \n",
    "    requires_hyperlink_d = {}\n",
    "    \n",
    "    n_rows = wk.rows + 1\n",
    "    for i in range(2, n_rows):\n",
    "        cell = pygsheets.Cell(f'A{i}', worksheet=wk, cell_data=None)\n",
    "\n",
    "        if not cell.formula and wk.get_value(f'A{i}'): # Must have value but no hyperlink\n",
    "\n",
    "            print(f'Cell A{i} requires a hyperlink: {wk.get_value(f\"A{i}\")}')\n",
    "            \n",
    "            cell_string = wk.get_value(f'A{i}')\n",
    "            case_name = cell_string.split('.')[0] + '_case.pptx' # Name to help locate file names\n",
    "            requires_hyperlink_d[case_name] = f'A{i}'\n",
    "            \n",
    "    return requires_hyperlink_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:39:14.533327Z",
     "start_time": "2020-03-25T15:39:14.442530Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "import pygsheets\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import time\n",
    "\n",
    "# https://pygsheets.readthedocs.io/en/stable/worksheet.html\n",
    "def mysql_query(query='DEFAULT', verbose=False):\n",
    "    import sqlalchemy as sql\n",
    "    username = 'jyu'\n",
    "    password = 't4pu6[\"Hw+X@=~gL'\n",
    "    connection = '192.168.157.106'\n",
    "    database_name = 'ods'\n",
    "    connect_string = f'mysql://{username}:{password}@{connection}/{database_name}'\n",
    "    sql_engine = sql.create_engine(connect_string)\n",
    "    df = pd.read_sql_query(query, sql_engine)\n",
    "    if verbose: print(df.shape)\n",
    "    return df\n",
    "\n",
    "def gsheet_import_caselist(network, verbose=False):\n",
    "    # Relating network to sheet index\n",
    "    network_dict = {\n",
    "        \"Sam's Club\":0,\n",
    "        'Walmart':1\n",
    "    }\n",
    "    network_choice = network_dict[network]\n",
    "    \n",
    "    gc = pygsheets.authorize(client_secret='client_secret.json')\n",
    "    sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1wsnBd3AHObl4gnUJ2MlwkusuXRoOsQpu2Kx6zGH8bVY/edit#gid=0')\n",
    "    df = sh[network_choice].get_as_df()\n",
    "    \n",
    "    if verbose: print(df.shape)\n",
    "    \n",
    "    print(f'Successfully returned {network}.')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gsheet_programs(gsheet_df):\n",
    "    print(f'Imported Gsheet Dataframe has (rows, columns): {gsheet_df.shape}')\n",
    "    not_list = gsheet_df['Program'].to_list() + ['0', '0']\n",
    "    not_tuple = tuple(not_list)\n",
    "    return not_tuple\n",
    "\n",
    "def sc_sql_metrics(not_tuple):\n",
    "    query = f\"\"\"\n",
    "    SELECT metrics.program, metrics.weeks_post, metrics.start_date, metrics.end_date, \n",
    "    metrics.item_number, metrics.item_description, metrics.category, metrics.subcategory,\n",
    "    metrics.category_num, metrics.subcat_num, metrics.Freeosk_clubs, metrics.control_clubs,\n",
    "    metrics.transactions, metrics.scans, metrics.audience, metrics.engagement, metrics.conversion,\n",
    "    metrics.repeat, metrics.Immediate, metrics.`HH_A%%`, metrics.`HH_B%%`, metrics.`HH_C%%`,\n",
    "    lift.freeosk_lift, lift.control_lift, cfg.merchandising_type, cfg.placement_type\n",
    "    FROM longitudinal.c_metrics as metrics\n",
    "    JOIN longitudinal.c_lift as lift ON metrics.Program = lift.program_name\n",
    "    JOIN longitudinal.c_cfg as cfg on metrics.program = cfg.program_name\n",
    "    WHERE metrics.Weeks_post = '12W'\n",
    "    AND metrics.program NOT IN {not_tuple}\n",
    "    AND metrics.program LIKE '%%_12.zip%%';\n",
    "    \"\"\"\n",
    "    raw_sc_df = mysql_query(query)\n",
    "    print(f'Number of SC rows to append: {len(raw_sc_df.index)}')\n",
    "    return raw_sc_df\n",
    "\n",
    "def sc_formatter(raw_sc_df):\n",
    "    # Formatting\n",
    "    print(f'Successfully returned the MySQL query')\n",
    "    sc_df = raw_sc_df.copy()\n",
    "    sc_df['Traffic/club'] = sc_df['audience'] / sc_df['Freeosk_clubs']\n",
    "    sc_df['Scans/club'] = sc_df['scans'] / sc_df['Freeosk_clubs']\n",
    "    sc_df['Program2'] = sc_df['program'].str.split(\"_\", n=1, expand=True)[0]\n",
    "    sc_df['Unique Identifier'] = sc_df['Program2'] + sc_df['item_number'].astype(str)\n",
    "    renamed_columns = {\n",
    "        'HH_A%': 'A%',\n",
    "        'HH_B%': 'B%',\n",
    "        'HH_C%': 'C%'\n",
    "    }\n",
    "    sc_df = sc_df.rename(columns=renamed_columns)\n",
    "    sc_df.loc[:, 'Tags'] = ''\n",
    "    sc_df.loc[:, 'Notes'] = ''\n",
    "    sc_df.columns = [column.replace('_', ' ') for column in list(sc_df)]\n",
    "    sc_df.columns = [column.capitalize() for column in list(sc_df)]\n",
    "    print(f'Successfully formatted the MySQL query')\n",
    "    \n",
    "    # Mapping to food boolean\n",
    "    food_mapping = pd.read_excel('data/Food_Mapping.xlsx')\n",
    "    sc_df_2 = pd.merge(sc_df, food_mapping,  how='left', \n",
    "                       left_on=['Category num','Subcat num'], \n",
    "                       right_on = ['CATEGORY_NUMBER','SUB_CATEGORY_NUMBER'])\n",
    "    print(f'Successfully joined to Food_Mapping.xslx')\n",
    "    \n",
    "    # Final column rearrangement\n",
    "    rearranged_columns = ['Program', 'Tags', 'Notes', 'Weeks post', 'Start date', \n",
    "                      'End date', 'Item number', 'Item description', 'Category', \n",
    "                      'Subcategory', 'Category num', 'Subcat num', 'Freeosk clubs', \n",
    "                      'Control clubs', 'Transactions', 'Audience', 'Traffic/club', \n",
    "                      'Scans', 'Scans/club', 'Engagement', 'Conversion', 'Repeat', \n",
    "                      'Immediate', 'A%', 'B%', 'C%', 'Freeosk lift', 'Control lift', \n",
    "                      'Merchandising type', 'Placement type', 'Program2', 'Unique identifier', 'Food']\n",
    "    \n",
    "    sc_df_3 = sc_df_2[rearranged_columns].reset_index(drop=True)\n",
    "    \n",
    "    print(f'SC_Formatter completed! Shape is {sc_df_3.shape}')\n",
    "    \n",
    "    return sc_df_3  \n",
    "\n",
    "def wm_sql_metrics(not_tuple):\n",
    "    query = f\"\"\"SELECT traceable.Program_name, \n",
    "    traceable.Weeks_post, traceable.Start_date, traceable.End_date,\n",
    "    traceable.Item_number, traceable.Placement_Name, traceable.Dept,\n",
    "    traceable.Sub_Dept, traceable.Dept_Name, traceable.Sub_Dept_Name,\n",
    "    traceable.Freeosk_stores, traceable.Rest_of_Chain_Stores,\n",
    "    traceable.Traceable_Audience, traceable.Non_Traceable_Aud,\n",
    "    traceable.Audience, traceable.Scans, traceable.Total_Conv,\n",
    "    traceable.Repeat, traceable.A, traceable.B, traceable.C,\n",
    "    traceable.Households, traceable.`A%%`, traceable.`B%%`, traceable.`C%%`, traceable.`A+B%%`,\n",
    "    lift.Control_lift, lift.Freeosk_lift\n",
    "    FROM longitudinal.traceable as traceable\n",
    "    JOIN longitudinal.lift as lift ON traceable.program_name = lift.Program_name\n",
    "    WHERE traceable.Weeks_post = '4W'\n",
    "    AND traceable.program_name LIKE '%%_4.zip'\n",
    "    AND traceable.program_name NOT IN {not_tuple};\"\"\"\n",
    "    raw_wm_df = mysql_query(query)\n",
    "    print(f'Number of WM rows to append: {len(raw_wm_df.index)}')\n",
    "    return raw_wm_df\n",
    "\n",
    "def wm_formatter(raw_wm_df):\n",
    "    # Formatting\n",
    "    print(f'Successfully returned the MySQL query')\n",
    "    wm_df = raw_wm_df.copy()\n",
    "    wm_df['Traffic/store'] = wm_df['Audience'] / wm_df['Freeosk_stores']\n",
    "    wm_df['Scans/store'] = wm_df['Scans'] / wm_df['Freeosk_stores']\n",
    "    wm_df['Program2'] = wm_df['Program_name'].str.split(\"_\", n=1, expand=True)[0]\n",
    "    wm_df.loc[:, 'Tags'] = ''\n",
    "    wm_df.loc[:, 'Notes'] = ''\n",
    "    wm_df.columns = [column.replace('_', ' ') for column in list(wm_df)]\n",
    "    wm_df.columns = [column.capitalize() for column in list(wm_df)]\n",
    "    print(f'Successfully formatted the MySQL query')\n",
    "    \n",
    "    # Final column rearrangement\n",
    "    rearranged_columns = ['Program name', 'Tags', 'Notes', 'Weeks post', 'Start date', 'End date', \n",
    "                          'Item number', 'Placement name', 'Dept', 'Sub dept', 'Dept name', \n",
    "                          'Sub dept name', 'Freeosk stores', 'Rest of chain stores', 'Traceable audience', \n",
    "                          'Non traceable aud', 'Audience', 'Traffic/store', 'Scans', 'Scans/store', 'Total conv', \n",
    "                          'Repeat', 'A', 'B', 'C', 'Households', 'A%', 'B%', 'C%', 'A+b%', 'Control lift', \n",
    "                          'Freeosk lift', 'Program2']\n",
    "    \n",
    "    wm_df = wm_df[rearranged_columns].reset_index(drop=True)\n",
    "    \n",
    "    rename_col = {'A+b%':'A+B%', 'Program name': 'Program'}\n",
    "    wm_df = wm_df.rename(columns=rename_col)\n",
    "    print(f'WM_Formatter completed! Shape is {wm_df.shape}')\n",
    "    \n",
    "    return wm_df\n",
    "\n",
    "def gsheet_uploader(network, gsheet_df, df):\n",
    "    gsheet_import_appended = gsheet_df.append(df, ignore_index=True)\n",
    "    \n",
    "    network_dict = {\n",
    "        \"Sam's Club\":0,\n",
    "        'Walmart':1\n",
    "    }\n",
    "    network_choice = network_dict[network]\n",
    "    \n",
    "    gc = pygsheets.authorize(client_secret='client_secret.json')\n",
    "    sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1wsnBd3AHObl4gnUJ2MlwkusuXRoOsQpu2Kx6zGH8bVY/edit#gid=0')\n",
    "    sh[network_choice].clear('A2')\n",
    "    sh[network_choice].set_dataframe(gsheet_import_appended, 'A2', copy_index=False, copy_head=False, extend=False, fit=True, escape_formulae=True, nan='')\n",
    "    print('New data has been successfully uploaded!') \n",
    "\n",
    "def refresher(network_input):\n",
    "    \n",
    "    if network_input == \"Sam's Club\" or network_input == 'All':\n",
    "        print(\"Running for Sam's Club\")\n",
    "        gsheet_df = gsheet_import_caselist(\"Sam's Club\")\n",
    "        not_tuple = gsheet_programs(gsheet_df)\n",
    "        raw_sc_df = sc_sql_metrics(not_tuple)\n",
    "        if raw_sc_df.empty: print('No new SC rows to append. Stopping upload.')\n",
    "        else: \n",
    "            sc_df = sc_formatter(raw_sc_df)\n",
    "            gsheet_uploader(\"Sam's Club\", gsheet_df, sc_df)\n",
    "    \n",
    "    print('------------------------------------------------')\n",
    "\n",
    "    if network_input == 'Walmart' or network_input == 'All':\n",
    "        print(\"Running for Walmart\")\n",
    "        gsheet_df = gsheet_import_caselist('Walmart')\n",
    "        not_tuple = gsheet_programs(gsheet_df)\n",
    "        raw_wm_df = wm_sql_metrics(not_tuple)\n",
    "        if raw_wm_df.empty: print('No new WM rows to append. Stopping upload.')\n",
    "        else: \n",
    "            wm_df = wm_formatter(raw_wm_df)\n",
    "            gsheet_uploader(\"Walmart\", gsheet_df, wm_df)\n",
    "\n",
    "class hyperlink_update:\n",
    "    \n",
    "    \n",
    "    def __init__(self, gsheet_url, network):\n",
    "        self.gsheet_url = gsheet_url\n",
    "        self.network = network\n",
    "        self.gc = pygsheets.authorize(client_secret='../client_secret.json')\n",
    "        self.sh = self.gc.open_by_url(gsheet_url)\n",
    "\n",
    "    def worksheet(self):\n",
    "        network_dict = {\n",
    "        \"Sam's Club\":0,\n",
    "        'Walmart':1\n",
    "        }\n",
    "        \n",
    "        # Relating network to sheet index\n",
    "        if self.network == 'All': network_choice =2\n",
    "        else: network_choice = network_dict[self.network]\n",
    "        network_choice = 2 # testing\n",
    "\n",
    "        self.wk = self.sh[network_choice]\n",
    "        \n",
    "    def hyperlink_retriever(self): # Retrieves list of programs to find in Cases GDrive folder\n",
    "        self.requires_hyperlink_d = {}\n",
    "        print('hello!')\n",
    "        n_rows = self.wk.rows + 1\n",
    "        for i in range(2, n_rows):\n",
    "            \n",
    " \n",
    "\n",
    "#             if i %% 5 == 0: time.sleep(100)\n",
    "            \n",
    "            cell = pygsheets.Cell(f'A{i}', worksheet=self.wk, cell_data=None)\n",
    "            print('hello!'+str(i))\n",
    "            cell_value = self.wk.get_value(f'A{i}')\n",
    "            \n",
    "            if not cell.formula and cell_value: # Must have value but no hyperlink\n",
    "\n",
    "                print(f'Cell A{i} requires a hyperlink: {cell_value}')\n",
    "                \n",
    "                file_name = cell_value.split('.')[0] + '_case.pptx' # Name to help locate file names\n",
    "                \n",
    "                # {File Name PPTX: Cell Location}\n",
    "                self.requires_hyperlink_d[file_name] = f'A{i}'\n",
    "                print('hello!FINAL' + str(i))\n",
    "                      \n",
    "    def file_list_agg(self):\n",
    "#         from pydrive.auth import GoogleAuth\n",
    "#         from pydrive.drive import GoogleDrive\n",
    "\n",
    "#         gauth = GoogleAuth()\n",
    "#         gauth.LocalWebserverAuth()\n",
    "\n",
    "#         drive = GoogleDrive(gauth)\n",
    "#         self.file_list = {}\n",
    "\n",
    "#         folder_files = drive.ListFile({'q': \"'1qQZpAhzmKoR7drBYh8kYWVS6GaMPCxr3' in parents\"}).GetList()\n",
    "#         for file in folder_files:\n",
    "#             print(file['title'])\n",
    "            \n",
    "#             # {'File Name PPTX': 'File GDrive ID'}\n",
    "#             self.file_list[file['title']] = file['id']\n",
    "\n",
    "    # If modifying these scopes, delete the file token.pickle.\n",
    "        SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly']\n",
    "\n",
    "        \"\"\"Shows basic usage of the Drive v3 API.\n",
    "        Prints the names and ids of the first 10 files the user has access to.\n",
    "        \"\"\"\n",
    "        creds = None\n",
    "        # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "        # created automatically when the authorization flow completes for the first\n",
    "        # time.\n",
    "        if os.path.exists('token.pickle'):\n",
    "            with open('token.pickle', 'rb') as token:\n",
    "                creds = pickle.load(token)\n",
    "        # If there are no (valid) credentials available, let the user log in.\n",
    "        if not creds or not creds.valid:\n",
    "            if creds and creds.expired and creds.refresh_token:\n",
    "                creds.refresh(Request())\n",
    "            else:\n",
    "                flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                    '../credentials.json', SCOPES)\n",
    "                creds = flow.run_local_server(port=0)\n",
    "            # Save the credentials for the next run\n",
    "            with open('token.pickle', 'wb') as token:\n",
    "                pickle.dump(creds, token)\n",
    "\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "        # Call the Drive v3 API\n",
    "        results = service.files().list(q=\"'1qQZpAhzmKoR7drBYh8kYWVS6GaMPCxr3' in parents\",\n",
    "                                                  spaces='drive',\n",
    "            pageSize=100, fields=\"nextPageToken, files(id, name)\").execute()\n",
    "        items = results.get('files', [])\n",
    "\n",
    "        self.file_list = {}\n",
    "\n",
    "        if not items:\n",
    "            print('No files found.')\n",
    "        else:\n",
    "            print('Files:')\n",
    "            for item in items:\n",
    "                print(u'{0} ({1})'.format(item['name'], item['id']))\n",
    "                self.file_list[file['title']] = file['id']\n",
    "        print(file_list)\n",
    "    \n",
    "    def file_name_checker(self):\n",
    "        print('IM IN ')\n",
    "        self.hyperlink_d = {}\n",
    "        \n",
    "        for file_name, file_id in self.file_list.items():\n",
    "            \n",
    "            if file_name in self.requires_hyperlink_d.keys():\n",
    "                print('FILE NAME' + file_name)\n",
    "                print(f'Found one {file_name}!')\n",
    "        \n",
    "                file_name_zip = file_name.split('.pptx')[0] + '.zip' # Reference back to original name\n",
    "            \n",
    "                # {'Cell location' : {'File Name Zipped': 'File GDrive ID'}}\n",
    "                self.hyperlink_d[self.requires_hyperlink_d[file_name]] = {file_name_zip:file_id} \n",
    "    \n",
    "    def hyperlink_updater(self):\n",
    "        for cell_location, inner_d in self.hyperlink_d.items():\n",
    "            file_name_zip = str(list(inner_d.keys())[0])\n",
    "            cell = pygsheets.Cell(cell_location, worksheet=self.wk)\n",
    "\n",
    "    def full_run(self):\n",
    "        if self.network == \"Sam's Club\" or self.network == 'All':\n",
    "            network_choice = 2\n",
    "            hyperlink_update.worksheet()\n",
    "            hyperlink_retriever()\n",
    "            file_list_agg()\n",
    "            file_name_checker()\n",
    "            hyperlink_updater()\n",
    "        if self.network == \"Walmart\" or self.network == 'All':\n",
    "            network_choice = 2\n",
    "            worksheet()\n",
    "            hyperlink_retriever()\n",
    "            file_list_agg()\n",
    "            file_name_checker()\n",
    "            hyperlink_updater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:39:39.882304Z",
     "start_time": "2020-03-25T15:39:36.244788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello!\n",
      "hello!2\n",
      "Cell A2 requires a hyperlink: FSC19077_980101251_20191213_12_case.zip\n",
      "hello!FINAL2\n",
      "hello!3\n",
      "hello!4\n",
      "hello!5\n",
      "hello!6\n",
      "hello!7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ModuleNotFoundError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\__init__.py\", line 41, in autodetect\n",
      "    try:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 41, in <module>\n",
      "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:\n",
      "Case List (1-p5-secff5mixTwj2QSqd7rl4twcmZBmQBaR1LrM2Pw)\n",
      "FSC19077_980101251_20191213_12_case.pptx (1LEdWvZrjGyWf1YepJL9lILLCyRwT0b5x)\n",
      "exampleslides (114cTE0IE5s5jsFTa9rWa0nOlerkH7HmuluUiLx6LcbU)\n",
      "{'Case List': '1-p5-secff5mixTwj2QSqd7rl4twcmZBmQBaR1LrM2Pw', 'FSC19077_980101251_20191213_12_case.pptx': '1LEdWvZrjGyWf1YepJL9lILLCyRwT0b5x', 'exampleslides': '114cTE0IE5s5jsFTa9rWa0nOlerkH7HmuluUiLx6LcbU'}\n",
      "SUCESFULLY\n",
      "IM IN \n"
     ]
    }
   ],
   "source": [
    "gsheet_url = 'https://docs.google.com/spreadsheets/d/1wsnBd3AHObl4gnUJ2MlwkusuXRoOsQpu2Kx6zGH8bVY/edit#gid=0'\n",
    "# network_name = initial_prompts('Case List Refresher')\n",
    "network_name = 'All'\n",
    "hyperlink = hyperlink_update(gsheet_url, network_name)\n",
    "hyperlink.worksheet()\n",
    "hyperlink.hyperlink_retriever()\n",
    "hyperlink.file_list_agg()\n",
    "print('SUCESFULLY')\n",
    "hyperlink.file_name_checker()\n",
    "hyperlink.hyperlink_updater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T14:50:37.912171Z",
     "start_time": "2020-03-25T14:50:31.907639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=558424667520-5gc5fj76ro74gnj8q4e6ff0e7lm2cu33.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ModuleNotFoundError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n",
      "Case List\n",
      "FSC19077_980101251_20191213_12_case.pptx\n",
      "exampleslides\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "\n",
    "drive = GoogleDrive(gauth)\n",
    "file_list = {}\n",
    "\n",
    "folder_files = drive.ListFile({'q': \"'1qQZpAhzmKoR7drBYh8kYWVS6GaMPCxr3' in parents\"}).GetList()\n",
    "for file in folder_files:\n",
    "    print(file['title'])\n",
    "\n",
    "    # {'File Name PPTX': 'File GDrive ID'}\n",
    "    file_list[file['title']] = file['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:08:39.084986Z",
     "start_time": "2020-03-25T15:08:38.402779Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ModuleNotFoundError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\__init__.py\", line 41, in autodetect\n",
      "    try:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 41, in <module>\n",
      "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:\n",
      "Case List (1-p5-secff5mixTwj2QSqd7rl4twcmZBmQBaR1LrM2Pw)\n",
      "FSC19077_980101251_20191213_12_case.pptx (1LEdWvZrjGyWf1YepJL9lILLCyRwT0b5x)\n",
      "exampleslides (114cTE0IE5s5jsFTa9rWa0nOlerkH7HmuluUiLx6LcbU)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# If modifying these scopes, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly']\n",
    "\n",
    "def main():\n",
    "    \"\"\"Shows basic usage of the Drive v3 API.\n",
    "    Prints the names and ids of the first 10 files the user has access to.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                '../credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    # Call the Drive v3 API\n",
    "    results = service.files().list(q=\"'1qQZpAhzmKoR7drBYh8kYWVS6GaMPCxr3' in parents\",\n",
    "                                              spaces='drive',\n",
    "        pageSize=100, fields=\"nextPageToken, files(id, name)\").execute()\n",
    "    items = results.get('files', [])\n",
    "\n",
    "    if not items:\n",
    "        print('No files found.')\n",
    "    else:\n",
    "        print('Files:')\n",
    "        for item in items:\n",
    "            print(u'{0} ({1})'.format(item['name'], item['id']))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T21:59:30.438481Z",
     "start_time": "2020-03-20T21:59:30.048054Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#     cell.formula = f'=HYPERLINK(\"https://drive.google.com/file/d/{file_id}\", \"{file_name_zip}\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T21:36:32.124004Z",
     "start_time": "2020-03-20T21:36:32.112036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found one FSC19077_980101251_20191213_12_case.pptx!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A2': {'FSC19077_980101251_20191213_12_case.zip': '1LEdWvZrjGyWf1YepJL9lILLCyRwT0b5x'}}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "hyperlink_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T21:17:56.341217Z",
     "start_time": "2020-03-20T21:17:53.292432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell A2 requires a hyperlink: FSC19077_980101251_20191213_12.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FSC19077_980101251_20191213_12_case.pptx': 'A2'}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requires_hyperlink_d = hyperlink_retriever('Walmart')\n",
    "requires_hyperlink_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T21:18:50.417485Z",
     "start_time": "2020-03-20T21:18:50.411502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FSC19077_980101251_20191213_12_case.pptx': '1LEdWvZrjGyWf1YepJL9lILLCyRwT0b5x',\n",
       " 'exampleslides': '114cTE0IE5s5jsFTa9rWa0nOlerkH7HmuluUiLx6LcbU'}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T21:19:33.910400Z",
     "start_time": "2020-03-20T21:19:33.903906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['FSC19077_980101251_20191213_12_case.pptx'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T21:01:35.771130Z",
     "start_time": "2020-03-20T21:01:24.327805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSC19077_980101251_20191213_12_case.pptx\n",
      "exampleslides\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FSC19077_980101251_20191213_12_case.pptx': '1LEdWvZrjGyWf1YepJL9lILLCyRwT0b5x',\n",
       " 'exampleslides': '114cTE0IE5s5jsFTa9rWa0nOlerkH7HmuluUiLx6LcbU'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T21:03:57.431617Z",
     "start_time": "2020-03-20T21:03:57.423584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A2': ['FSC19077_980101251_20191213_12_case.pptx']}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T20:23:14.025119Z",
     "start_time": "2020-03-20T20:23:07.820445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=558424667520-5gc5fj76ro74gnj8q4e6ff0e7lm2cu33.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T19:35:26.425277Z",
     "start_time": "2020-03-20T19:35:23.847174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND A NOT FORMULATED for A2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A2': ['FSC19077_980101251_20191213_12_case.pptx', 'hello']}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "d = {}\n",
    "\n",
    "    \n",
    "for i in range(2, n_rows):\n",
    "    cell = pygsheets.Cell(f'A{i}', worksheet=sh[2], cell_data=None)\n",
    "    \n",
    "    if not cell.formula and sh[2].get_value(f'A{i}'): # Cannot be formulated but must have value\n",
    "        \n",
    "        print(f'A{i} requires a formula')\n",
    "        \n",
    "        case_name = sh[2].get_value(f'A{i}').split('.')[0] + '_case.pptx'\n",
    "        d[f'A{i}'] = [case_name]\n",
    "        \n",
    "#         d[f'A{i}'].append('hello')\n",
    "    \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T19:01:25.458771Z",
     "start_time": "2020-03-20T19:01:25.451788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1LEdWvZrjGyWf1YepJL9lILLCyRwT0b5x'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://drive.google.com/file/d/1LEdWvZrjGyWf1YepJL9lILLCyRwT0b5x/view\"\n",
    "casefile_id = url.split('https://drive.google.com/file/d/')[1].split('/view')[0]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
